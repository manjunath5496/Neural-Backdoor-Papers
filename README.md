# Neural Backdoor Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(1).pdf" style="text-decoration:none;">Debugging Machine Learning Tasks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(2).pdf" style="text-decoration:none;">Neural Trojans</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(3).pdf" style="text-decoration:none;">Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(4).pdf" style="text-decoration:none;">Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(5).pdf" style="text-decoration:none;">Hu-Fu: Hardware and Software Collaborative Attack Framework against Neural Networks</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(6).pdf" style="text-decoration:none;">Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(7).pdf" style="text-decoration:none;">Hardware Trojan Attacks on Neural Networks</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(8).pdf" style="text-decoration:none;"> Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(9).pdf" style="text-decoration:none;">Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(10).pdf" style="text-decoration:none;">STRIP: A Defence Against Trojan Attacks on Deep Neural Networks </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(11).pdf" style="text-decoration:none;">Design of Intentional Backdoors In Sequential Models</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(12).pdf" style="text-decoration:none;">A new Backdoor Attack in CNNs by training set corruption without label poisoning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(13).pdf" style="text-decoration:none;">Quantitative Verification of Neural Networks And Its Security Applications</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(14).pdf" style="text-decoration:none;">Universal Litmus Patterns:
Revealing Backdoor Attacks in CNNs</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(15).pdf" style="text-decoration:none;">TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(16).pdf" style="text-decoration:none;">Model Agnostic Defence against Backdoor Attacks in Machine Learning</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(17).pdf" style="text-decoration:none;">Februus: Input Purification Defense Against Trojan Attacks on Deep Neural Network Systems</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(18).pdf" style="text-decoration:none;">Invisible Backdoor Attacks on Deep Neural Networks via Steganography and Regularization</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(19).pdf" style="text-decoration:none;">Hidden Trigger Backdoor Attacks</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(20).pdf" style="text-decoration:none;">Detecting AI Trojans
Using Meta Neural Analysis</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(21).pdf" style="text-decoration:none;">A Survey on Neural Trojans</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(22).pdf" style="text-decoration:none;">Backdoor Attacks against Learning Systems</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(23).pdf" style="text-decoration:none;">Latent Backdoor Attacks on Deep Neural Networks</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(24).pdf" style="text-decoration:none;">Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(25).pdf" style="text-decoration:none;">BadNets: Evaluating Backdooring Attacks on Deep Neural Networks</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(26).pdf" style="text-decoration:none;">Detecting Poisoning Attacks on Machine Learning in IoT Environments</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(27).pdf" style="text-decoration:none;">ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(28).pdf" style="text-decoration:none;">DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(29).pdf" style="text-decoration:none;">Sensitive-Sample Fingerprinting of Deep Neural Networks </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(30).pdf" style="text-decoration:none;">SIN2: Stealth Infection on Neural Network â€“ A Low-cost Agile Neural Trojan Attack Methodology</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(31).pdf" style="text-decoration:none;">Trojaning Attack on Neural Networks</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(32).pdf" style="text-decoration:none;">Watermarking Deep Neural Networks for Embedded Systems</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Neural-Backdoor-Papers/blob/master/nb(33).pdf" style="text-decoration:none;">Gotta Catch 'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks</a></li>                              

  </ul>
